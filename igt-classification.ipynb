{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae900efd-050a-48c2-8f62-135468c942a5",
   "metadata": {},
   "source": [
    "# 1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a9a4c020-4ec6-4824-8a8d-9ae5e713d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datapreparation import IGTClassificationDataset\n",
    "\n",
    "labeled_data_path = \"labeled_data.json\"\n",
    "model_name = \"nlpaueb/legal-bert-base-uncased\"\n",
    "general_output_path = \"./results\"\n",
    "model_output_path = \"./igtclassification-model\"\n",
    "label_map_fname = \"label_map.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03778563-fd1c-4038-bc27-1292430778e1",
   "metadata": {},
   "source": [
    "# 2. Load labeled data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12ce4ea4-1648-4463-a594-629979492994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_json(json_file_path):\n",
    "    # Lists to store all examples\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Read and parse the JSON file\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Iterate through each entry in the JSON array\n",
    "    for entry in data:\n",
    "        # Extract tokens and labels for this entry\n",
    "        tokens = entry[\"tokens\"]\n",
    "        labels = entry[\"labels\"]\n",
    "        \n",
    "        # Add to our collected lists\n",
    "        all_tokens.append(tokens)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    return all_tokens, all_labels\n",
    "\n",
    "json_file_path = labeled_data_path\n",
    "tokens_list, labels_list = load_data_from_json(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3693df-22b7-480f-8d38-e10bfcca38c7",
   "metadata": {},
   "source": [
    "# 3. Split data into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ce838b30-219b-459a-81d6-03523eeaa2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, val_tokens, train_labels, val_labels = train_test_split(\n",
    "    tokens_list, labels_list, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_dataset = IGTClassificationDataset(train_tokens, train_labels, tokenizer)\n",
    "val_dataset = IGTClassificationDataset(val_tokens, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ecbacd-ca16-4388-9280-47e0f84e408a",
   "metadata": {},
   "source": [
    "# 4. Initialize pretrained model (LegalBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "02b2d52a-8db3-4056-b1dc-74e8d8f52bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide warnings\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "num_labels = len(train_dataset.get_label_map())\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f03658-4fd7-42d6-b47b-f0787ce092a1",
   "metadata": {},
   "source": [
    "# 5. Train and save fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9f387ea2-7fb6-4956-8379-31df99bd6fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokens and labels have different lengths at index 10\n",
      "Tokens: 34, Labels: 33\n",
      "{'eval_loss': 2.6780357360839844, 'eval_runtime': 0.1605, 'eval_samples_per_second': 49.858, 'eval_steps_per_second': 6.232, 'epoch': 1.0}\n",
      "Warning: Tokens and labels have different lengths at index 10\n",
      "Tokens: 34, Labels: 33\n",
      "{'eval_loss': 2.678057909011841, 'eval_runtime': 0.0745, 'eval_samples_per_second': 107.425, 'eval_steps_per_second': 13.428, 'epoch': 2.0}\n",
      "Warning: Tokens and labels have different lengths at index 10\n",
      "Tokens: 34, Labels: 33\n",
      "{'eval_loss': 2.678065299987793, 'eval_runtime': 0.0726, 'eval_samples_per_second': 110.202, 'eval_steps_per_second': 13.775, 'epoch': 3.0}\n",
      "Warning: Tokens and labels have different lengths at index 10\n",
      "Tokens: 34, Labels: 33\n",
      "{'eval_loss': 2.6780707836151123, 'eval_runtime': 0.0726, 'eval_samples_per_second': 110.14, 'eval_steps_per_second': 13.767, 'epoch': 4.0}\n",
      "Warning: Tokens and labels have different lengths at index 10\n",
      "Tokens: 34, Labels: 33\n",
      "{'eval_loss': 2.6780970096588135, 'eval_runtime': 0.0733, 'eval_samples_per_second': 109.097, 'eval_steps_per_second': 13.637, 'epoch': 5.0}\n",
      "Warning: Tokens and labels have different lengths at index 10\n",
      "Tokens: 34, Labels: 33\n",
      "{'eval_loss': 2.6780989170074463, 'eval_runtime': 0.0746, 'eval_samples_per_second': 107.2, 'eval_steps_per_second': 13.4, 'epoch': 6.0}\n",
      "Warning: Tokens and labels have different lengths at index 10\n",
      "Tokens: 34, Labels: 33\n",
      "{'eval_loss': 2.6781108379364014, 'eval_runtime': 0.0736, 'eval_samples_per_second': 108.684, 'eval_steps_per_second': 13.585, 'epoch': 7.0}\n",
      "Warning: Tokens and labels have different lengths at index 10\n",
      "Tokens: 34, Labels: 33\n",
      "{'eval_loss': 2.6781246662139893, 'eval_runtime': 0.0728, 'eval_samples_per_second': 109.901, 'eval_steps_per_second': 13.738, 'epoch': 8.0}\n",
      "Warning: Tokens and labels have different lengths at index 10\n",
      "Tokens: 34, Labels: 33\n",
      "{'eval_loss': 2.6781394481658936, 'eval_runtime': 0.0724, 'eval_samples_per_second': 110.501, 'eval_steps_per_second': 13.813, 'epoch': 9.0}\n",
      "Warning: Tokens and labels have different lengths at index 10\n",
      "Tokens: 34, Labels: 33\n",
      "{'eval_loss': 2.6781318187713623, 'eval_runtime': 0.0744, 'eval_samples_per_second': 107.508, 'eval_steps_per_second': 13.439, 'epoch': 10.0}\n",
      "Warning: Tokens and labels have different lengths at index 10\n",
      "Tokens: 34, Labels: 33\n",
      "{'eval_loss': 2.6781301498413086, 'eval_runtime': 0.0727, 'eval_samples_per_second': 110.038, 'eval_steps_per_second': 13.755, 'epoch': 11.0}\n",
      "Warning: Tokens and labels have different lengths at index 10\n",
      "Tokens: 34, Labels: 33\n",
      "{'eval_loss': 2.6781301498413086, 'eval_runtime': 0.0725, 'eval_samples_per_second': 110.279, 'eval_steps_per_second': 13.785, 'epoch': 12.0}\n",
      "Warning: Tokens and labels have different lengths at index 10\n",
      "Tokens: 34, Labels: 33\n",
      "{'eval_loss': 2.678129196166992, 'eval_runtime': 0.1186, 'eval_samples_per_second': 67.462, 'eval_steps_per_second': 8.433, 'epoch': 13.0}\n",
      "{'train_runtime': 9.3484, 'train_samples_per_second': 22.25, 'train_steps_per_second': 1.391, 'train_loss': 2.6549151493952823, 'epoch': 13.0}\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=general_output_path,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-7,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=13,\n",
    "    weight_decay=0.05,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save model and tokenizer in a clean format\n",
    "trainer.model.save_pretrained(model_output_path)\n",
    "tokenizer.save_pretrained(model_output_path)\n",
    "\n",
    "# Save the label mapping for inference\n",
    "with open(f\"{model_output_path}/{label_map_fname}\", \"w\") as f:\n",
    "    json.dump(train_dataset.get_label_map(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaeea47-cf26-4b1b-8071-5f18c6fecb79",
   "metadata": {},
   "source": [
    "# 6. Load model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f570343d-15d9-4867-b92c-148d66aad50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Load label mapping\n",
    "    with open(f\"{model_path}/{label_map_fname}\", \"r\") as f:\n",
    "        label_map = json.load(f)\n",
    "        \n",
    "    # Create reverse mapping (id to label)\n",
    "    id_to_label = {int(idx): label for label, idx in label_map.items()}\n",
    "    \n",
    "    return model, tokenizer, id_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f16af60-ebce-4317-9fe2-727e26fef605",
   "metadata": {},
   "source": [
    "# 7. Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0b3dba1-bd38-4df8-be04-efd0cf8f3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence_tags(text, model, tokenizer, id_to_label):\n",
    "    \"\"\"\n",
    "    Predict sequence tags for input text using a fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to be tagged\n",
    "        model: Fine-tuned token classification model\n",
    "        tokenizer: Tokenizer corresponding to the model\n",
    "        id_to_label (dict): Mapping from label IDs to label names\n",
    "        \n",
    "    Returns:\n",
    "        list: List of (token, label) tuples\n",
    "    \"\"\"\n",
    "    # Use the tokenizer to split into tokens first\n",
    "    nlp_tokens = text.split()  # We use simple splitting first to get base tokens\n",
    "    \n",
    "    # Then get BERT tokens and word_ids mapping\n",
    "    encoding = tokenizer(\n",
    "        nlp_tokens,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Map predictions back to original tokens\n",
    "    predicted_labels = []\n",
    "    previous_word_idx = None\n",
    "    \n",
    "    # Use word_ids to map predictions back to original tokens\n",
    "    word_ids = encoding.word_ids(batch_index=0)\n",
    "    \n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        # Skip special tokens and tokens that we've already assigned labels to\n",
    "        if word_idx is None or word_idx == previous_word_idx:\n",
    "            continue\n",
    "            \n",
    "        # Get prediction for this token (use the first subword's prediction)\n",
    "        pred_id = predictions[0, idx].item()\n",
    "        \n",
    "        # Map prediction ID to label name\n",
    "        if pred_id in id_to_label:\n",
    "            label = id_to_label[pred_id]\n",
    "        else:\n",
    "            label = \"O\"  # Default to Outside tag\n",
    "            \n",
    "        # Add prediction\n",
    "        predicted_labels.append(label)\n",
    "        previous_word_idx = word_idx\n",
    "    \n",
    "    # Return tokens with their predicted labels\n",
    "    return list(zip(nlp_tokens, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084cc878-775b-4450-91bd-51d6930f40cc",
   "metadata": {},
   "source": [
    "# 8. Prediction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "507c8226-e7dd-4d3e-8f06-8fbe922dc380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\tLabel\n",
      "------------------------------\n",
      "If\tB-Or_Else\n",
      "a\tI-Activation_Condition\n",
      "pandemic\tB-Aim\n",
      "happens,\tB-Aim\n",
      "the\tB-Aim\n",
      "WHO\tB-Or_Else\n",
      "should\tI-Activation_Condition\n",
      "administer\tI-Activation_Condition\n",
      "vaccines\tB-Aim\n",
      "before\tB-Aim\n",
      "30\tI-Or_Else\n",
      "days\tB-Activation_Condition\n",
      "has\tI-Activation_Condition\n",
      "expired,\tI-Or_Else\n",
      "failing\tI-Execution_Constraint\n",
      "which\tB-Aim\n",
      "they\tB-Or_Else\n",
      "will\tI-Activation_Condition\n",
      "be\tI-Execution_Constraint\n",
      "imposed\tB-Aim\n",
      "a\tI-Activation_Condition\n",
      "fine\tI-Execution_Constraint\n",
      "of\tB-Aim\n",
      "twenty\tI-Execution_Constraint\n",
      "million\tB-Aim\n",
      "euros.\tB-Aim\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "model, tokenizer, id_to_label = load_model(model_output_path)\n",
    "\n",
    "# Test it on a sample sentence\n",
    "test_sentence = \"If a pandemic happens, the WHO should administer vaccines before 30 days has expired, failing which they will be imposed a fine of twenty million euros.\"\n",
    "results = predict_sequence_tags(test_sentence, model, tokenizer, id_to_label)\n",
    "\n",
    "# Print results in a readable format\n",
    "print(\"Token\\tLabel\")\n",
    "print(\"-\" * 30)\n",
    "for token, label in results:\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd309ea-2fe4-471c-9439-2578b3fc6089",
   "metadata": {},
   "source": [
    "# 9. Visualise prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dc9613ae-90d3-4bba-8e03-4db147c84379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def print_colored_results(results):\n",
    "    \"\"\"Print results with color-coding using HTML for Jupyter notebooks and include a legend.\"\"\"\n",
    "    \n",
    "    color_map = {\n",
    "        \"B-Activation_Condition\": \"red\",\n",
    "        \"I-Activation_Condition\": \"red\",\n",
    "        \"B-Attribute\": \"blue\",\n",
    "        \"I-Attribute\": \"blue\",\n",
    "        \"B-Deontic\": \"green\",\n",
    "        \"I-Deontic\": \"green\",\n",
    "        \"B-Aim\": \"orange\",\n",
    "        \"I-Aim\": \"orange\",\n",
    "        \"B-Object\": \"magenta\",\n",
    "        \"I-Object\": \"magenta\",\n",
    "        \"B-Execution_Constraint\": \"cyan\",\n",
    "        \"I-Execution_Constraint\": \"cyan\",\n",
    "        \"B-Or_Else\": \"lightcoral\",\n",
    "        \"I-Or_Else\": \"lightcoral\",\n",
    "        \"O\": \"black\"\n",
    "    }\n",
    "\n",
    "    # Generate the legend\n",
    "    legend_html = \"<p><b>Legend:</b></p><table style='border-collapse: collapse;'>\"\n",
    "    for tag, color in color_map.items():\n",
    "        legend_html += f\"\"\"\n",
    "        <tr>\n",
    "            <td style='background-color: {color}; width: 20px; height: 20px;'></td>\n",
    "            <td style='padding-left: 10px;'>{tag}</td>\n",
    "        </tr>\n",
    "        \"\"\"\n",
    "    legend_html += \"</table><br>\"\n",
    "\n",
    "    # Generate the colored text output\n",
    "    text_html = \"\"\n",
    "    for token, label in results:\n",
    "        color = color_map.get(label, \"black\")\n",
    "        text_html += f'<span style=\"color: {color}; font-weight: bold;\">{token}</span> '\n",
    "\n",
    "    # Display legend and text\n",
    "    display(HTML(legend_html + text_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7ae3f-6c85-4224-a495-8d44030b280e",
   "metadata": {},
   "source": [
    "# 10. Visualisation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66711493-1e98-44b9-90d9-c5c6060481be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Legend:</b></p><table style='border-collapse: collapse;'>\n",
       "        <tr>\n",
       "            <td style='background-color: red; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>B-Activation_Condition</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: red; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>I-Activation_Condition</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: blue; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>B-Attribute</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: blue; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>I-Attribute</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: green; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>B-Deontic</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: green; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>I-Deontic</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: orange; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>B-Aim</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: orange; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>I-Aim</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: magenta; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>B-Object</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: magenta; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>I-Object</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: cyan; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>B-Execution_Constraint</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: cyan; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>I-Execution_Constraint</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: lightcoral; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>B-Or_Else</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: lightcoral; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>I-Or_Else</td>\n",
       "        </tr>\n",
       "        \n",
       "        <tr>\n",
       "            <td style='background-color: black; width: 20px; height: 20px;'></td>\n",
       "            <td style='padding-left: 10px;'>O</td>\n",
       "        </tr>\n",
       "        </table><br><span style=\"color: lightcoral; font-weight: bold;\">If</span> <span style=\"color: red; font-weight: bold;\">a</span> <span style=\"color: orange; font-weight: bold;\">pandemic</span> <span style=\"color: orange; font-weight: bold;\">happens,</span> <span style=\"color: orange; font-weight: bold;\">the</span> <span style=\"color: lightcoral; font-weight: bold;\">WHO</span> <span style=\"color: red; font-weight: bold;\">should</span> <span style=\"color: red; font-weight: bold;\">administer</span> <span style=\"color: orange; font-weight: bold;\">vaccines</span> <span style=\"color: orange; font-weight: bold;\">before</span> <span style=\"color: lightcoral; font-weight: bold;\">30</span> <span style=\"color: red; font-weight: bold;\">days</span> <span style=\"color: red; font-weight: bold;\">has</span> <span style=\"color: lightcoral; font-weight: bold;\">expired,</span> <span style=\"color: cyan; font-weight: bold;\">failing</span> <span style=\"color: orange; font-weight: bold;\">which</span> <span style=\"color: lightcoral; font-weight: bold;\">they</span> <span style=\"color: red; font-weight: bold;\">will</span> <span style=\"color: cyan; font-weight: bold;\">be</span> <span style=\"color: orange; font-weight: bold;\">imposed</span> <span style=\"color: red; font-weight: bold;\">a</span> <span style=\"color: cyan; font-weight: bold;\">fine</span> <span style=\"color: orange; font-weight: bold;\">of</span> <span style=\"color: cyan; font-weight: bold;\">twenty</span> <span style=\"color: orange; font-weight: bold;\">million</span> <span style=\"color: orange; font-weight: bold;\">euros.</span> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visual output\n",
    "print_colored_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igtenv",
   "language": "python",
   "name": "igtenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
